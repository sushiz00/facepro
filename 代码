import os
import csv
import numpy as np
from PIL import Image
# 载入数据集
database_path = r'D:\data\fer2013'  # 数据集路径
datasets_path = r'D:\data\fer2013'  # 输出路径
csv_file = os.path.join(database_path, 'fer2013.csv')  # 数据集
train_csv = os.path.join(datasets_path, 'train.csv')  # 训练集
val_csv = os.path.join(datasets_path, 'val.csv')  # 验证集
test_csv = os.path.join(datasets_path, 'test.csv')  # 测试集

# 分离训练集、验证集和测试集
with open(csv_file) as f:
    csvr = csv.reader(f)  # 按行读取返回行列表
    header = next(csvr)  # 获取第一行标题
    rows = [row for row in csvr]  # 遍历每行

    # 按最后一列的标签将数据集进行分割   第一列row[:-1]，最后一列row[-1]
    trn = [row[:-1] for row in rows if row[-1] == 'Training']
    csv.writer(open(train_csv, 'w+'), lineterminator='\n').writerows([header[:-1]] + trn)
    print("训练集的数量为：", len(trn))

    val = [row[:-1] for row in rows if row[-1] == 'PublicTest']
    csv.writer(open(val_csv, 'w+'), lineterminator='\n').writerows([header[:-1]] + val)
    print("验证集的数量为：", len(val))

    tst = [row[:-1] for row in rows if row[-1] == 'PrivateTest']
    csv.writer(open(test_csv, 'w+'), lineterminator='\n').writerows([header[:-1]] + tst)
    print("测试集的数量为：", len(tst))


# 将分开的三个数据集转化为单通道灰度图，同时按照表情进行分类
datasets_path = r'D:\data\fer2013'
train_csv = os.path.join(datasets_path, 'train.csv')  # 获取数据
val_csv = os.path.join(datasets_path, 'val.csv')
test_csv = os.path.join(datasets_path, 'test.csv')
train_set = os.path.join(datasets_path, 'train')  # 输出图片
val_set = os.path.join(datasets_path, 'val')
test_set = os.path.join(datasets_path, 'test')

for save_path, csv_file in [(train_set, train_csv), (val_set, val_csv), (test_set, test_csv)]:
    if not os.path.exists(save_path):  # 保存文件夹不存在则创建
        os.makedirs(save_path)

    num = 1
    with open(csv_file) as f:
        csvr = csv.reader(f)
        header = next(csvr)

        # 使用enumerate遍历csvr中的标签(label)和特征值（pixel)
        for i, (label, pixel) in enumerate(csvr):
            # 将特征值的数组转化为48*48的矩阵
            pixel = np.asarray([float(p) for p in pixel.split()]).reshape(48, 48)
            subfolder = os.path.join(save_path, label)
            if not os.path.exists(subfolder):
                os.makedirs(subfolder)
            # 将该矩阵转化为RGB图像，再通过convert转化为8位灰度图像，L指灰度图模式，L=R*299/1000+G*587/1000+B*114/1000
            img = Image.fromarray(pixel).convert('L')
            image_name = os.path.join(subfolder, '{:05d}.jpg'.format(i))
            print(image_name)
            img.save(image_name)


  import os
from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

datagen = ImageDataGenerator(
    rotation_range=20,  # 旋转范围
    width_shift_range=0.1,  # 水平平移范围
    height_shift_range=0.1,  # 垂直平移范围
    shear_range=0.1,  # 透视变换的范围
    zoom_range=0.1,  # 缩放范围
    horizontal_flip=True,  # 水平反转
    fill_mode='nearest')

dir = 'D:\data\fer2013/train/1'  # 数据增强文件路径
for filename in os.listdir(dir):
    print(filename)
    img = load_img(dir + '/' + filename)  # 这是一个PIL图像
    x = img_to_array(img)  # 把PIL图像转换成一个numpy数组，形状为(3, 150, 150)
    x = x.reshape((1,) + x.shape)  # 这是一个numpy数组，形状为 (1, 3, 150, 150)
    # 下面是生产图片的代码
    i = 0
    for batch in datagen.flow(x, batch_size=1,
                              save_to_dir='D:\data\fer2013/train/1',
                              save_prefix='1',
                              save_format='jpeg'):
        i += 1
        if i > 5:
            break  # 否则生成器会退出循环


  import os
import pickle
from tensorflow import optimizers
from keras.models import Sequential
from matplotlib import pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten, Conv2D

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# 生成器读取图像
train_dir = r'D:\data\dataaug\train'
val_dir = r'D:\data\dataset\val'
test_dir = r'D:\data\dataset\test'

train_datagen = ImageDataGenerator(
    rescale=1. / 255,  # 重放缩因子，数值乘以1.0/255（归一化）
    shear_range=0.2,  # 剪切强度（逆时针方向的剪切变换角度）
    zoom_range=0.2,  # 随机缩放的幅度
    horizontal_flip=True  # 进行随机水平翻转
)
val_datagen = ImageDataGenerator(rescale=1. / 255)
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(48, 48),
    batch_size=128,
    shuffle=True,
    class_mode='categorical'
)
validation_generator = test_datagen.flow_from_directory(
    val_dir,
    target_size=(48, 48),
    batch_size=128,
    shuffle=True,
    class_mode='categorical'
)
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(48, 48),
    batch_size=128,
    shuffle=True,
    class_mode='categorical'
)

# 构建网络
model = Sequential()
# 第一段
# 第一卷积层，64个大小为5×5的卷积核，步长1，激活函数relu，卷积模式same，输入张量的大小
model.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same', input_shape=(48, 48, 3)))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))  # 第一池化层，池化核大小为2×2，步长2
model.add(BatchNormalization())
model.add(Dropout(0.4))  # 随机丢弃40%的网络连接，防止过拟合
# 第二段
model.add(Conv2D(128, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.4))
# 第三段
model.add(Conv2D(256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Flatten())  # 过渡层
model.add(Dropout(0.3))
model.add(Dense(2048, activation='relu'))  # 全连接层
model.add(Dropout(0.4))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(512, activation='relu'))
model.add(Dense(7, activation='softmax'))  # 分类输出层
model.summary()

# 编译
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.Adam(),  # Adam优化器
              # optimizer=optimizers.RMSprop(learning_rate=0.0001),  # rmsprop优化器
              metrics=['accuracy'])

# 训练模型
history = model.fit(
    train_generator,  # 生成训练集生成器
    steps_per_epoch=243,  # train_num/batch_size=128
    epochs=40,  # 数据迭代轮数
    validation_data=validation_generator,  # 生成验证集生成器
    validation_steps=28  # valid_num/batch_size=128
)

# 评估模型
test_loss, test_acc = model.evaluate(test_generator, steps=28)
print("test_loss: %.4f - test_acc: %.4f" % (test_loss, test_acc * 100))

# 保存模型
model_json = model.to_json()
with open('myModel_2_json.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights('myModel_2_weight.h5')
model.save('myModel_2.h5')

with open('fit_2_log.txt', 'wb') as file_txt:
    pickle.dump(history.history, file_txt, 0)

# 绘制训练中的损失曲线和精度曲线
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure("acc")
plt.plot(epochs, acc, 'r-', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='validation acc')
plt.title('Accuracy curve')
plt.legend()
plt.savefig('acc_2.jpg')
plt.show()

plt.figure("loss")
plt.plot(epochs, loss, 'r-', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('Loss curve')
plt.legend()
plt.savefig('loss_2.jpg')
plt.show()
